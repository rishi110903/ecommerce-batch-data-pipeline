# E-Commerce Batch Data Pipeline

## Overview
This project implements an end-to-end batch ETL pipeline that processes raw e-commerce transaction data and converts it into analytics-ready tables. The workflow is orchestrated using Apache Airflow and executed inside Docker containers.

The pipeline demonstrates a real data warehouse flow:
Raw transactions → Cleaned staging data → Dimension tables → Fact table

---

## Tech Stack
- Python
- MySQL
- Apache Airflow
- Docker & Docker Compose
- SQL

---

## Pipeline Workflow
The Airflow DAG runs three tasks:

1. **ingest_raw_data**  
   Loads CSV transaction data into MySQL (`raw_orders`) using Python.

2. **transform_staging**  
   Cleans and standardizes records into a staging table (`stg_orders`).

3. **build_analytics**  
   Creates warehouse tables:
   - `dim_customer`
   - `dim_product`
   - `fact_orders`

---

## Data Model
The warehouse follows a **star schema**.

- **raw_orders** → raw transactional data  
- **stg_orders** → cleaned and typed records  
- **dim_customer** → unique customers  
- **dim_product** → unique products  
- **fact_orders** → final analytics table (one row per order)

This structure enables fast aggregation and reporting.

---

## Project Structure
dags/ Airflow DAG definition
scripts/ Python ingestion script
sql/ SQL transformations
data/raw/ Input dataset
airflow-docker/ Docker setup for Airflow

---

## Running the Project

1. Clone the repository

2. Start Airflow
cd airflow-docker
docker compose up airflow-webserver airflow-scheduler

3. Open Airflow UI  
http://localhost:8080

Enable and trigger the DAG: `ecommerce_batch_pipeline`

---

## What This Project Demonstrates
- Batch ETL pipeline design
- Workflow orchestration with Airflow
- SQL data transformation
- Star schema modeling
- Analytics-ready dataset preparation
