# E-Commerce Batch Data Pipeline

## Overview
This project implements an end-to-end batch data engineering pipeline for e-commerce data using **Python, MySQL, Apache Airflow, and Docker**.

The pipeline ingests raw CSV data, applies transformations, and builds analytics-ready fact and dimension tables.

---

## Architecture

CSV (Raw Data)
→ Python Ingestion
→ MySQL (Raw Layer)
→ SQL Transformations (Staging + Analytics)
→ MySQL (Fact & Dimension Tables)
→ Orchestrated with Apache Airflow (Dockerized)


---

## Tech Stack
- Python (pandas, mysql-connector)
- MySQL
- SQL
- Apache Airflow
- Docker & Docker Compose

---

## Pipeline Steps

### 1. Ingestion
- Reads raw e-commerce CSV data  
- Loads data into `raw_orders` table  

### 2. Staging
- Cleans and standardizes raw data  
- Creates `stg_orders`  

### 3. Analytics Layer
- Builds dimension tables:
  - `dim_customer`
  - `dim_product`
- Builds fact table:
  - `fact_orders`

### 4. Orchestration
- Automated using Airflow DAG  
- Tasks run sequentially with retries  

---

## How to Run

### Prerequisites
- Docker Desktop
- MySQL running locally

### Setup

1. Clone the repository  
2. Create a `.env` file inside `airflow-docker/`:

```env
PROJECT_ROOT=your_project_path
MYSQL_HOST=host.docker.internal
MYSQL_USER=root
MYSQL_PASSWORD=your_password
MYSQL_DATABASE=de_batch_pipeline

## start airflow:
docker compose up airflow-webserver airflow-scheduler

## Open Airflow UI:
http://localhost:8080

Then trigger the DAG
